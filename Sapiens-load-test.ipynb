{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data2/hongn/sapiens/cv/mmcv/cnn/bricks/transformer.py:38: UserWarning: Fail to import ``MultiScaleDeformableAttention`` from ``mmcv.ops.multi_scale_deform_attn``, You should install ``mmcv`` rather than ``mmcv-lite`` if you need this module. \n",
      "  warnings.warn('Fail to import ``MultiScaleDeformableAttention`` from '\n",
      "/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# from extract_feature import *\n",
    "from mmpretrain import get_model\n",
    "import os\n",
    "import mmengine\n",
    "device = 'cuda:0'\n",
    "config = \"/data2/hongn/sapiens/pretrain/configs/sapiens_mae/humans_300m_test/mae_sapiens_0.3b-p16_8xb512-coslr-1600e_humans_300m_test.py\"\n",
    "checkpoint = \"/data2/hongn/sapiens/pretrain/checkpoints/sapiens_0.3b/sapiens_0.3b_epoch_1600_clean.pth\"\n",
    "input = \"/data2/hongn/sapiens/pose/demo/data/itw_videos\"\n",
    "output_root = \"/data2/hongn/sapiens/outputs/ados2_pretrain\"\n",
    "\n",
    "if output_root:\n",
    "    mmengine.mkdir_or_exist(output_root)\n",
    "    output_file = os.path.join(output_root, os.path.basename(input))\n",
    "\n",
    "# inferencer = FeatureExtractor(model=config, pretrained=checkpoint, device='cuda', backbone=dict(out_indices=(0, 1, 2, 3, 4)))\n",
    "inferencer = get_model(model=config, pretrained=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4097, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dummy_video = torch.randn(1, 3, 1024, 1024).to(device)\n",
    "feats = inferencer.extract_feat(dummy_video)\n",
    "feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.diag(torch.ones(4), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tridiag(a, b, c, k1=-1, k2=0, k3=1):\n",
    "    return np.diag(a, k1) + np.diag(b, k2) + np.diag(c, k3)\n",
    "\n",
    "a = [1, 1]; b = [1, 1, 1]; c = [1, 1]\n",
    "A = tridiag(a, b, c)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 1., 1.],\n",
       "        [1., 2., 1., 1.],\n",
       "        [1., 1., 2., 1.],\n",
       "        [1., 1., 1., 2.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.diag(torch.ones(4), 0) + torch.ones(4,1)  #+ torch.diag(torch.ones(3), 1) + torch.diag(torch.ones(3), -1)\n",
    "A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def arrow(a, b, c, k1=-1, k2=0, k3=1):\n",
    "    return np.diag(a-1, k1) + np.diag(b, k2) + np.diag(c, k3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(3)[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1., 1., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def arrow_matrix(size = 8, n = 2):\n",
    "    dim = size - 1\n",
    "    ret = torch.diag(torch.ones(dim), 0)\n",
    "    for j in range(n):\n",
    "        for i in range(dim-n-j, dim-j):\n",
    "            ret = ret + torch.diag(torch.ones(i), dim-i) + torch.diag(torch.ones(i), i-dim)\n",
    "    row = torch.ones(dim, 1)\n",
    "    col = torch.ones(1, size)\n",
    "\n",
    "    return torch.concat((col,torch.concat((row, ret), axis=1)), axis=0)\n",
    "\n",
    "A = arrow_matrix(size = 8, n = 1)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1., 1., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = torch.ones(7, 1)\n",
    "col = torch.ones(1, 8)\n",
    "B = torch.concat((row, A), axis=1)\n",
    "torch.concat((col, B), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 10\n",
    "n = 1\n",
    "img_len=5\n",
    "ret = torch.diag(torch.ones(dim), 0)\n",
    "# for i in range(dim-n, dim):\n",
    "#     ret = ret + torch.diag(torch.ones(i), dim-i) + torch.diag(torch.ones(i), i-dim)\n",
    "\n",
    "ret = ret + torch.diag(torch.ones(9-5), 10-9-7)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zebra_matrix(dim = 6, n = 2):\n",
    "    ret = torch.diag(torch.ones(dim), 0)\n",
    "    for i in range(dim-n, dim):\n",
    "        ret = ret + torch.diag(torch.ones(i), dim-i) + torch.diag(torch.ones(i), i-dim)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndiag_matrix(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False, False],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True],\n",
       "          [ True,  True,  True,  True]]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "visual_mask = torch.tensor([[0, 1, 1, 1]]).bool()\n",
    "motion_mask = torch.tensor([[1, 1, 1,1]]).bool() \n",
    "\n",
    "attn_mask = rearrange(visual_mask, 'b i -> b 1 i 1') * rearrange(motion_mask, 'b j -> b 1 1 j')\n",
    "\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ndiag_matrix(5,1).unsqueeze(0).repeat(3, 1, 1).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 0.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,1,0,0,0]]).bool()\n",
    "b = torch.tensor([[1,1,0,0]]).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True,  True, False, False],\n",
       "          [ True,  True, False, False],\n",
       "          [False, False, False, False],\n",
       "          [False, False, False, False],\n",
       "          [False, False, False, False]]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_mask = rearrange(a, 'b i -> b 1 i 1') * rearrange(b, 'b j -> b 1 1 j')\n",
    "attn_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapiens_lite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
